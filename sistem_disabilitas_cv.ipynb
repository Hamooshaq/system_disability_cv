{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"dataset/train\"\n",
    "test_folder = \"dataset/test\"\n",
    "valid_folder = \"dataset/valid\"\n",
    "\n",
    "train_annotation = os.path.join(train_folder, \"_annotations.csv\")\n",
    "test_annotation = os.path.join(test_folder, \"_annotations.csv\")\n",
    "valid_annotation = os.path.join(valid_folder, \"_annotations.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Annotations:\n",
      "                                           filename  width  height  \\\n",
      "0  101_jpeg.rf.0a1b2b7c9c5dc78d68e338c00628d072.jpg    640     640   \n",
      "1  149_jpeg.rf.12791a08af9b198a4975c55cc72bd61c.jpg    640     640   \n",
      "2  140_jpeg.rf.02c767f81d5d7d18f9c211d633a58f6f.jpg    640     640   \n",
      "3  140_jpeg.rf.02c767f81d5d7d18f9c211d633a58f6f.jpg    640     640   \n",
      "4  140_jpeg.rf.02c767f81d5d7d18f9c211d633a58f6f.jpg    640     640   \n",
      "\n",
      "             class  xmin  ymin  xmax  ymax  \n",
      "0  old-aged-person    22     3   400   640  \n",
      "1  old-aged-person    26    70   309   489  \n",
      "2  old-aged-person   464     3   622   303  \n",
      "3  old-aged-person   335    34   442   297  \n",
      "4  old-aged-person   221     5   338   224  \n"
     ]
    }
   ],
   "source": [
    "def load_annotations(csv_file):\n",
    "    return pd.read_csv(csv_file)\n",
    "\n",
    "train_annotations = load_annotations(train_annotation)\n",
    "test_annotations = load_annotations(test_annotation)\n",
    "valid_annotations = load_annotations(valid_annotation)\n",
    "\n",
    "print(\"Train Annotations:\")\n",
    "print(train_annotations.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasett.yaml created at: dataset/datasett.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "base_path = 'dataset'\n",
    "\n",
    "train_path = os.path.join(base_path, 'train', 'image')\n",
    "valid_path = os.path.join(base_path, 'valid', 'image')\n",
    "test_path = os.path.join(base_path, 'test', 'image')\n",
    "\n",
    "classes = ['old-aged-person', 'cane', 'wheelchair']\n",
    "\n",
    "data_dict = {\n",
    "    'train': train_path,\n",
    "    'val': valid_path,\n",
    "    'test': test_path,\n",
    "    'nc': len(classes),\n",
    "    'names': classes,\n",
    "}\n",
    "\n",
    "yaml_file_path = os.path.join(base_path, 'datasett.yaml')\n",
    "with open(yaml_file_path, 'w') as file:\n",
    "    yaml.dump(data_dict, file)\n",
    "\n",
    "print(f'datasett.yaml created at: {yaml_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'dataset/valid/_annotations.csv'\n",
    "image_folder = 'dataset/valid/image'\n",
    "output_label_folder = 'dataset/valid/image'\n",
    "\n",
    "os.makedirs(output_label_folder, exist_ok=True)\n",
    "\n",
    "class_mapping = {\n",
    "    'old-aged-person': 0,\n",
    "    'cane': 1,\n",
    "    'wheelchair': 2\n",
    "}\n",
    "\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete.\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    filename = row['filename']\n",
    "    width = row['width']\n",
    "    height = row['height']\n",
    "    class_name = row['class']\n",
    "    xmin = row['xmin']\n",
    "    ymin = row['ymin']\n",
    "    xmax = row['xmax']\n",
    "    ymax = row['ymax']\n",
    "\n",
    "    x_center = (xmin + xmax) / 2 / width\n",
    "    y_center = (ymin + ymax) / 2 / height\n",
    "    bbox_width = (xmax - xmin) / width\n",
    "    bbox_height = (ymax - ymin) / height\n",
    "\n",
    "    class_id = class_mapping.get(class_name, 'unknown')\n",
    "    \n",
    "    if class_id == 'unknown':\n",
    "        continue\n",
    "\n",
    "    yolo_annotation = f\"{class_id} {x_center} {y_center} {bbox_width} {bbox_height}\\n\"\n",
    "    \n",
    "    label_filename = os.path.join(output_label_folder, os.path.splitext(filename)[0] + '.txt')\n",
    "    \n",
    "    with open(label_filename, 'a') as f:\n",
    "        f.write(yolo_annotation)\n",
    "\n",
    "print(\"Conversion complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "output_augmented_folder = 'dataset/valid/augmented_image'\n",
    "\n",
    "os.makedirs(output_augmented_folder, exist_ok=True)\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Blur(blur_limit=3, p=0.2),\n",
    "    A.ColorJitter(p=0.2),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.5)\n",
    "])\n",
    "\n",
    "image_path = os.path.join(image_folder, filename)\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "augmented = transform(image=image)\n",
    "augmented_image = augmented['image']\n",
    "\n",
    "augmented_image_filename = f\"aug_{filename}\"\n",
    "augmented_image_path = os.path.join(output_augmented_folder, augmented_image_filename)\n",
    "cv2.imwrite(augmented_image_path, augmented_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.11 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.2 ðŸš€ Python-3.12.2 torch-2.4.1 CPU (Apple M2)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataset/datasett_augmented.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train59, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train59\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    431257  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "Model summary: 249 layers, 2,690,793 parameters, 2,690,777 gradients, 6.9 GFLOPs\n",
      "\n",
      "Transferred 313/391 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train59', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/hamooshaq/projek/sistem_disabilitas1/dataset/train/image.cache... 113 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 114/114 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image.cache... 33 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/103_jpeg.rf.a8c06e583716f458c1ae5bc7e0374fdd.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/107_jpeg.rf.2293c0d43492848eb9f626c605b6d246.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/109_jpeg.rf.c184b440f35cef017cbeaac2f6f8afec.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/110_jpeg.rf.04893295ed99d1911778b9310ff5be59.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/114_jpeg.rf.0aae0f80345417732f8f5e6b08d091db.jpg: 39 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/119_jpeg.rf.c29ecd9f6cc750a3e51de6a27251ca40.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/121_jpeg.rf.2b3ff32eddb6a774bd5a1c585baa2732.jpg: 78 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/123_jpeg.rf.9fd6838d801f5be5c2f2139e4c025051.jpg: 39 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/144_jpeg.rf.b3a95a86534881c4a5f5dfbe63743f58.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/150_jpeg.rf.65ef9145b79d4be397ca63895e4a7b7d.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/152_jpeg.rf.fc4906498dd0ec4c6f906e9979e6e31b.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/17_jpeg.rf.5601da4677e19eee1bff8d9d6b7dc319.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/23_jpeg.rf.feb2d28f1629abc857abf2b2ecf8dab6.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/32_jpeg.rf.6993b61aac465f4e83e2c69ea5312842.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/39_jpeg.rf.2297effa44fa0bf831a5432e8438c984.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/44_jpeg.rf.c9a444bc3c7f63210b48e4ecd7d3f5b5.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/48_jpeg.rf.3f4fad5aa37a5257db3e3defa738bb91.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/52_jpeg.rf.51b004cee4ddf72edb5a6dfe2956ed04.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/53_jpeg.rf.aa78b644e7e9a5b131b43415355fc68a.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/57_jpeg.rf.21150aa89f784ccaefc9b695c9ecaa99.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/61_jpeg.rf.b8bfffb9afc831cb90b74b5015787aff.jpg: 52 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/62_jpeg.rf.28e5a62d0e6bd40b507cce3f92eb9288.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/63_jpeg.rf.3e06c30bdb0ee135d6d3471d01aecf78.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/64_jpeg.rf.01dfee98423297ca105713960313e0be.jpg: 39 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/67_jpeg.rf.11f17972a28f66f0a70051f9b9d9867b.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/74_jpeg.rf.200e63b2068383484a104d61d9151528.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/76_jpeg.rf.75f8a111cb4bb9a41cb1b69b38133d50.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/79_jpeg.rf.64963a972281e44003ce5d943d9b2028.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/81_jpeg.rf.80e994ea65fb27e9c444566f54181de8.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/82_jpeg.rf.2775aff2b34352c6ff7aa7156e67182c.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/87_jpeg.rf.38b5c37b567a1ca67768a7a4738fd9b0.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/91_jpeg.rf.ee5c934bea07b7d6877f5d32019b86d2.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/th_jpeg.rf.8ec0ccd3a416b13ef0d17c9ca4b30f4c.jpg: 13 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train59/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 63 weight(decay=0.0), 70 weight(decay=0.0005), 69 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train59\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.863      3.596      1.997          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:45<00:00, 20.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58    0.00262      0.554     0.0358     0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G      1.732      3.561      1.956          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:10<00:00, 16.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58     0.0051      0.541     0.0517     0.0145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G       1.56      3.361      1.688          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:02<00:00, 15.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58    0.00431      0.613     0.0901     0.0299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G      1.546      3.167       1.64         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:01<00:00, 15.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58    0.00777      0.798      0.169     0.0569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G      1.527      2.977      1.673          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:08<00:00, 16.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58    0.00511      0.652      0.197     0.0905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G      1.602      2.699      1.651          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:08<00:00, 16.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58    0.00227      0.476      0.194     0.0845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G       1.56       2.51      1.634          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [05:01<00:00, 37.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58     0.0035      0.651      0.182     0.0755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G      1.519      2.461       1.62          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:09<00:00, 16.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.004      0.745      0.129      0.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G      1.594      2.425      1.654          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:25<00:00, 18.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.064      0.568      0.164      0.084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50         0G      1.662      2.453      1.716          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:27<00:00, 18.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58     0.0041      0.766      0.252      0.103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50         0G      1.541      2.355      1.663          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:58<00:00, 14.79s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.737     0.0167      0.105     0.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50         0G      1.526      2.268      1.647          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:12<00:00, 16.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.466     0.0266      0.282     0.0841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50         0G      1.448      2.143      1.616          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:54<00:00, 14.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.389      0.183      0.194     0.0565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50         0G      1.516       2.15      1.641          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:51<00:00, 13.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.584       0.35      0.386       0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50         0G      1.538      2.048      1.596          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:55<00:00, 14.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.334      0.267      0.289      0.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50         0G      1.595      2.195      1.686          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:50<00:00, 13.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.416      0.366       0.34      0.157\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50         0G      1.651      2.208      1.669         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:48<00:00, 13.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58        0.3      0.264       0.22      0.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50         0G      1.422      2.351      1.546          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:56<00:00, 14.56s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.364      0.287      0.315      0.135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50         0G      1.485      2.011      1.584          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:23<00:00, 17.88s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.228      0.193      0.208      0.113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50         0G      1.559      2.135      1.685          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:54<00:00, 21.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58       0.13      0.262      0.114     0.0457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50         0G      1.511      2.198      1.616          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:10<00:00, 16.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.422       0.27      0.253     0.0937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50         0G      1.416      1.917      1.483          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:04<00:00, 15.51s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.345      0.261      0.263       0.11\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50         0G      1.439      1.844      1.489          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:58<00:00, 14.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.299      0.425      0.334      0.156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50         0G      1.408      1.776       1.51          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:03<00:00, 15.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.389      0.511      0.388      0.155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50         0G      1.391      2.163      1.497          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:10<00:00, 16.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.373      0.321      0.226       0.08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50         0G      1.374      1.778        1.5          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:54<00:00, 14.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.187      0.248      0.205     0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50         0G      1.473       1.81      1.507         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:01<00:00, 15.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.421      0.588      0.391       0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50         0G      1.431       1.75       1.47          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:52<00:00, 14.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.481      0.653      0.596      0.245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50         0G      1.446      1.847      1.529         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:05<00:00, 15.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.584      0.599      0.612      0.295\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50         0G       1.44      1.849      1.558          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [02:04<00:00, 15.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.618      0.546      0.523      0.243\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50         0G      1.393      1.702      1.488          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:21<00:00, 10.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.569      0.554       0.51       0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50         0G      1.352      1.655      1.411          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:13<00:00,  9.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.491       0.53      0.544      0.297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50         0G      1.276      1.651      1.416         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:13<00:00,  9.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.523      0.525      0.569      0.275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50         0G      1.224      1.515      1.346          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:16<00:00,  9.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.466      0.621      0.543      0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50         0G      1.316       1.64      1.425          9        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:16<00:00,  9.57s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.658      0.523      0.558      0.225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50         0G      1.252      1.621       1.42          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:16<00:00,  9.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.534      0.609       0.52       0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50         0G      1.242      1.648      1.386          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:51<00:00,  6.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.522      0.615      0.552      0.257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50         0G      1.176       1.49      1.351          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:53<00:00,  6.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58       0.58      0.612      0.637      0.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50         0G      1.138       1.41      1.316          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:37<00:00, 12.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.545       0.67      0.661      0.362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50         0G      1.156      1.682      1.327          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [15:58<00:00, 119.79s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.576      0.666      0.691      0.347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50         0G      1.141      1.699      1.345          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:56<00:00,  7.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.613      0.674      0.659      0.342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50         0G      1.232       1.84      1.496          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:52<00:00,  6.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.636      0.542       0.63      0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50         0G      1.165      1.555      1.425          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:01<00:00,  7.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.593      0.589      0.615      0.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50         0G      1.014      1.556      1.271          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:51<00:00,  6.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.498      0.636      0.563      0.298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50         0G      1.097      1.545      1.431          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:01<00:00,  7.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.595      0.687      0.596      0.318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50         0G       1.11      1.451      1.291          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:03<00:00,  7.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.613      0.701      0.645       0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50         0G      1.065      1.557      1.351          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:59<00:00,  7.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.676      0.646      0.675      0.327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50         0G     0.9171      1.367      1.204          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:02<00:00,  7.86s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.761      0.586      0.688       0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50         0G     0.9857      1.474      1.302          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:06<00:00,  8.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.725      0.647      0.718      0.339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50         0G     0.9739      1.367      1.274          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [01:08<00:00,  8.57s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.798      0.647      0.729      0.334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 1.878 hours.\n",
      "Optimizer stripped from runs/detect/train59/weights/last.pt, 5.6MB\n",
      "Optimizer stripped from runs/detect/train59/weights/best.pt, 5.6MB\n",
      "\n",
      "Validating runs/detect/train59/weights/best.pt...\n",
      "Ultralytics 8.3.2 ðŸš€ Python-3.12.2 torch-2.4.1 CPU (Apple M2)\n",
      "Model summary (fused): 186 layers, 2,684,953 parameters, 0 gradients, 6.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:06<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.545       0.67      0.661      0.363\n",
      "       old-aged-person         18         31      0.598      0.452      0.493      0.164\n",
      "                  cane         15         20      0.479        0.7      0.655      0.354\n",
      "            wheelchair          7          7      0.559      0.857      0.836      0.571\n",
      "Speed: 1.1ms preprocess, 181.0ms inference, 0.0ms loss, 3.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train59\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "model.train(data='dataset/datasett_augmented.yaml', epochs=50)\n",
    "\n",
    "model.save('yolov8.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.2 ðŸš€ Python-3.12.2 torch-2.4.1 CPU (Apple M2)\n",
      "Model summary (fused): 186 layers, 2,684,953 parameters, 0 gradients, 6.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image.cache... 33 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/103_jpeg.rf.a8c06e583716f458c1ae5bc7e0374fdd.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/107_jpeg.rf.2293c0d43492848eb9f626c605b6d246.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/109_jpeg.rf.c184b440f35cef017cbeaac2f6f8afec.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/110_jpeg.rf.04893295ed99d1911778b9310ff5be59.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/114_jpeg.rf.0aae0f80345417732f8f5e6b08d091db.jpg: 39 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/119_jpeg.rf.c29ecd9f6cc750a3e51de6a27251ca40.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/121_jpeg.rf.2b3ff32eddb6a774bd5a1c585baa2732.jpg: 78 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/123_jpeg.rf.9fd6838d801f5be5c2f2139e4c025051.jpg: 39 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/144_jpeg.rf.b3a95a86534881c4a5f5dfbe63743f58.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/150_jpeg.rf.65ef9145b79d4be397ca63895e4a7b7d.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/152_jpeg.rf.fc4906498dd0ec4c6f906e9979e6e31b.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/17_jpeg.rf.5601da4677e19eee1bff8d9d6b7dc319.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/23_jpeg.rf.feb2d28f1629abc857abf2b2ecf8dab6.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/32_jpeg.rf.6993b61aac465f4e83e2c69ea5312842.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/39_jpeg.rf.2297effa44fa0bf831a5432e8438c984.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/44_jpeg.rf.c9a444bc3c7f63210b48e4ecd7d3f5b5.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/48_jpeg.rf.3f4fad5aa37a5257db3e3defa738bb91.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/52_jpeg.rf.51b004cee4ddf72edb5a6dfe2956ed04.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/53_jpeg.rf.aa78b644e7e9a5b131b43415355fc68a.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/57_jpeg.rf.21150aa89f784ccaefc9b695c9ecaa99.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/61_jpeg.rf.b8bfffb9afc831cb90b74b5015787aff.jpg: 52 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/62_jpeg.rf.28e5a62d0e6bd40b507cce3f92eb9288.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/63_jpeg.rf.3e06c30bdb0ee135d6d3471d01aecf78.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/64_jpeg.rf.01dfee98423297ca105713960313e0be.jpg: 39 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/67_jpeg.rf.11f17972a28f66f0a70051f9b9d9867b.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/74_jpeg.rf.200e63b2068383484a104d61d9151528.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/76_jpeg.rf.75f8a111cb4bb9a41cb1b69b38133d50.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/79_jpeg.rf.64963a972281e44003ce5d943d9b2028.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/81_jpeg.rf.80e994ea65fb27e9c444566f54181de8.jpg: 26 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/82_jpeg.rf.2775aff2b34352c6ff7aa7156e67182c.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/87_jpeg.rf.38b5c37b567a1ca67768a7a4738fd9b0.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/91_jpeg.rf.ee5c934bea07b7d6877f5d32019b86d2.jpg: 13 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /Users/hamooshaq/projek/sistem_disabilitas1/dataset/valid/image/th_jpeg.rf.8ec0ccd3a416b13ef0d17c9ca4b30f4c.jpg: 13 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:11<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         33         58      0.545       0.67      0.661      0.363\n",
      "       old-aged-person         18         31      0.598      0.452      0.493      0.164\n",
      "                  cane         15         20      0.479        0.7      0.655      0.354\n",
      "            wheelchair          7          7      0.559      0.857      0.836      0.571\n",
      "Speed: 3.1ms preprocess, 345.6ms inference, 0.0ms loss, 5.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train592\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.val()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 55.8ms\n",
      "Speed: 2.3ms preprocess, 55.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 (no detections), 68.3ms\n",
      "Speed: 1.8ms preprocess, 68.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 60.1ms\n",
      "Speed: 1.8ms preprocess, 60.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 55.9ms\n",
      "Speed: 2.2ms preprocess, 55.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 57.8ms\n",
      "Speed: 2.1ms preprocess, 57.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 57.7ms\n",
      "Speed: 2.7ms preprocess, 57.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 78.8ms\n",
      "Speed: 2.9ms preprocess, 78.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 81.6ms\n",
      "Speed: 3.4ms preprocess, 81.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 100.2ms\n",
      "Speed: 3.9ms preprocess, 100.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 91.2ms\n",
      "Speed: 3.0ms preprocess, 91.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 104.0ms\n",
      "Speed: 3.9ms preprocess, 104.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 99.9ms\n",
      "Speed: 4.8ms preprocess, 99.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 90.9ms\n",
      "Speed: 3.4ms preprocess, 90.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 92.7ms\n",
      "Speed: 3.0ms preprocess, 92.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 bowls, 80.9ms\n",
      "Speed: 3.4ms preprocess, 80.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bowl, 2 carrots, 90.9ms\n",
      "Speed: 3.1ms preprocess, 90.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 carrot, 1 laptop, 104.8ms\n",
      "Speed: 3.1ms preprocess, 104.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 knife, 94.7ms\n",
      "Speed: 3.1ms preprocess, 94.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 knife, 1 hot dog, 89.6ms\n",
      "Speed: 2.9ms preprocess, 89.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 77.8ms\n",
      "Speed: 2.7ms preprocess, 77.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 93.0ms\n",
      "Speed: 4.0ms preprocess, 93.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 114.4ms\n",
      "Speed: 3.2ms preprocess, 114.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 91.3ms\n",
      "Speed: 2.6ms preprocess, 91.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 118.1ms\n",
      "Speed: 3.3ms preprocess, 118.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 banana, 84.9ms\n",
      "Speed: 2.7ms preprocess, 84.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 100.6ms\n",
      "Speed: 2.9ms preprocess, 100.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 laptop, 78.0ms\n",
      "Speed: 2.9ms preprocess, 78.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 laptop, 79.7ms\n",
      "Speed: 3.2ms preprocess, 79.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 laptop, 89.8ms\n",
      "Speed: 3.0ms preprocess, 89.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 laptop, 83.5ms\n",
      "Speed: 3.1ms preprocess, 83.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 laptop, 80.3ms\n",
      "Speed: 2.9ms preprocess, 80.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 laptop, 75.7ms\n",
      "Speed: 2.8ms preprocess, 75.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 laptop, 69.9ms\n",
      "Speed: 3.3ms preprocess, 69.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 laptop, 88.2ms\n",
      "Speed: 3.1ms preprocess, 88.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 tv, 1 laptop, 80.5ms\n",
      "Speed: 2.9ms preprocess, 80.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cups, 2 laptops, 80.7ms\n",
      "Speed: 4.5ms preprocess, 80.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cups, 2 laptops, 65.8ms\n",
      "Speed: 3.2ms preprocess, 65.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cups, 1 laptop, 79.8ms\n",
      "Speed: 3.1ms preprocess, 79.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cups, 1 tv, 1 laptop, 79.5ms\n",
      "Speed: 2.7ms preprocess, 79.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 2 cups, 1 spoon, 1 bowl, 1 chair, 1 tv, 1 laptop, 77.1ms\n",
      "Speed: 3.3ms preprocess, 77.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 tv, 1 laptop, 89.9ms\n",
      "Speed: 3.1ms preprocess, 89.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 traffic light, 77.8ms\n",
      "Speed: 3.1ms preprocess, 77.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 traffic light, 69.0ms\n",
      "Speed: 2.7ms preprocess, 69.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 96.7ms\n",
      "Speed: 3.3ms preprocess, 96.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 81.4ms\n",
      "Speed: 3.7ms preprocess, 81.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 70.7ms\n",
      "Speed: 2.9ms preprocess, 70.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 81.1ms\n",
      "Speed: 3.2ms preprocess, 81.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 70.0ms\n",
      "Speed: 3.0ms preprocess, 70.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 1 umbrella, 1 laptop, 82.7ms\n",
      "Speed: 3.0ms preprocess, 82.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 tv, 1 laptop, 82.3ms\n",
      "Speed: 3.2ms preprocess, 82.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 tv, 85.2ms\n",
      "Speed: 3.1ms preprocess, 85.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 laptop, 78.1ms\n",
      "Speed: 2.9ms preprocess, 78.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 tv, 1 laptop, 75.8ms\n",
      "Speed: 3.0ms preprocess, 75.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 laptop, 73.4ms\n",
      "Speed: 2.8ms preprocess, 73.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 laptop, 77.5ms\n",
      "Speed: 3.8ms preprocess, 77.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 laptop, 81.8ms\n",
      "Speed: 3.0ms preprocess, 81.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 laptop, 76.7ms\n",
      "Speed: 2.8ms preprocess, 76.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 laptop, 59.7ms\n",
      "Speed: 2.6ms preprocess, 59.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 laptop, 73.9ms\n",
      "Speed: 3.1ms preprocess, 73.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 laptop, 71.9ms\n",
      "Speed: 3.7ms preprocess, 71.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 laptop, 74.9ms\n",
      "Speed: 6.5ms preprocess, 74.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 laptop, 83.7ms\n",
      "Speed: 3.2ms preprocess, 83.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 laptop, 76.1ms\n",
      "Speed: 3.1ms preprocess, 76.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 laptop, 65.6ms\n",
      "Speed: 3.7ms preprocess, 65.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 laptop, 80.5ms\n",
      "Speed: 4.0ms preprocess, 80.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 laptop, 69.0ms\n",
      "Speed: 3.5ms preprocess, 69.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 laptop, 75.2ms\n",
      "Speed: 3.1ms preprocess, 75.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 suitcase, 1 laptop, 75.5ms\n",
      "Speed: 3.1ms preprocess, 75.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 laptop, 84.8ms\n",
      "Speed: 2.9ms preprocess, 84.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 suitcase, 1 laptop, 81.4ms\n",
      "Speed: 2.8ms preprocess, 81.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 laptop, 75.1ms\n",
      "Speed: 3.0ms preprocess, 75.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 laptop, 66.8ms\n",
      "Speed: 3.2ms preprocess, 66.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 tv, 1 laptop, 80.0ms\n",
      "Speed: 3.8ms preprocess, 80.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 tie, 1 tv, 1 laptop, 76.3ms\n",
      "Speed: 3.8ms preprocess, 76.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 tie, 1 laptop, 72.3ms\n",
      "Speed: 2.9ms preprocess, 72.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 tie, 1 tv, 1 laptop, 70.8ms\n",
      "Speed: 3.5ms preprocess, 70.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 umbrella, 3 ties, 1 laptop, 70.2ms\n",
      "Speed: 3.5ms preprocess, 70.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2 ties, 1 laptop, 87.3ms\n",
      "Speed: 2.8ms preprocess, 87.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 tie, 1 laptop, 86.1ms\n",
      "Speed: 3.6ms preprocess, 86.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 79.4ms\n",
      "Speed: 3.0ms preprocess, 79.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 86.7ms\n",
      "Speed: 2.9ms preprocess, 86.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 75.0ms\n",
      "Speed: 3.9ms preprocess, 75.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 75.3ms\n",
      "Speed: 3.5ms preprocess, 75.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 124.6ms\n",
      "Speed: 3.9ms preprocess, 124.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 81.2ms\n",
      "Speed: 3.2ms preprocess, 81.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 scissors, 73.2ms\n",
      "Speed: 3.8ms preprocess, 73.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 scissors, 74.7ms\n",
      "Speed: 3.1ms preprocess, 74.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 77.7ms\n",
      "Speed: 3.8ms preprocess, 77.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 63.8ms\n",
      "Speed: 3.5ms preprocess, 63.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cat, 84.5ms\n",
      "Speed: 3.1ms preprocess, 84.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.5ms\n",
      "Speed: 2.9ms preprocess, 74.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 toilet, 67.3ms\n",
      "Speed: 3.0ms preprocess, 67.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 75.7ms\n",
      "Speed: 3.1ms preprocess, 75.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 80.9ms\n",
      "Speed: 3.1ms preprocess, 80.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.6ms\n",
      "Speed: 3.6ms preprocess, 77.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 70.0ms\n",
      "Speed: 3.5ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 79.0ms\n",
      "Speed: 3.9ms preprocess, 79.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bird, 1 umbrella, 78.9ms\n",
      "Speed: 3.0ms preprocess, 78.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 73.7ms\n",
      "Speed: 3.0ms preprocess, 73.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 81.8ms\n",
      "Speed: 3.4ms preprocess, 81.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bed, 72.4ms\n",
      "Speed: 3.9ms preprocess, 72.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 81.1ms\n",
      "Speed: 2.9ms preprocess, 81.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bed, 66.1ms\n",
      "Speed: 2.8ms preprocess, 66.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bed, 80.4ms\n",
      "Speed: 3.0ms preprocess, 80.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 75.2ms\n",
      "Speed: 3.9ms preprocess, 75.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 1 bed, 115.9ms\n",
      "Speed: 10.4ms preprocess, 115.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 suitcases, 1 bed, 95.6ms\n",
      "Speed: 4.0ms preprocess, 95.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bed, 89.8ms\n",
      "Speed: 2.9ms preprocess, 89.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bed, 77.3ms\n",
      "Speed: 3.0ms preprocess, 77.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 75.1ms\n",
      "Speed: 2.9ms preprocess, 75.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bed, 79.9ms\n",
      "Speed: 3.9ms preprocess, 79.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 1 bed, 77.6ms\n",
      "Speed: 3.8ms preprocess, 77.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 1 bed, 65.9ms\n",
      "Speed: 2.9ms preprocess, 65.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 86.9ms\n",
      "Speed: 3.3ms preprocess, 86.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 suitcases, 89.9ms\n",
      "Speed: 3.0ms preprocess, 89.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 suitcases, 79.2ms\n",
      "Speed: 3.0ms preprocess, 79.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 71.0ms\n",
      "Speed: 3.9ms preprocess, 71.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 83.1ms\n",
      "Speed: 3.7ms preprocess, 83.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 umbrella, 78.0ms\n",
      "Speed: 3.5ms preprocess, 78.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.6ms\n",
      "Speed: 4.1ms preprocess, 67.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 82.5ms\n",
      "Speed: 3.1ms preprocess, 82.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 toilet, 75.3ms\n",
      "Speed: 3.7ms preprocess, 75.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 65.1ms\n",
      "Speed: 3.2ms preprocess, 65.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 toothbrush, 76.2ms\n",
      "Speed: 3.1ms preprocess, 76.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 77.5ms\n",
      "Speed: 3.0ms preprocess, 77.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 69.4ms\n",
      "Speed: 4.2ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 78.6ms\n",
      "Speed: 2.9ms preprocess, 78.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bench, 67.3ms\n",
      "Speed: 3.3ms preprocess, 67.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bench, 81.5ms\n",
      "Speed: 2.9ms preprocess, 81.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bench, 1 chair, 76.1ms\n",
      "Speed: 3.1ms preprocess, 76.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 86.0ms\n",
      "Speed: 3.3ms preprocess, 86.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 76.0ms\n",
      "Speed: 3.5ms preprocess, 76.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 74.0ms\n",
      "Speed: 3.7ms preprocess, 74.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 chair, 1 book, 72.9ms\n",
      "Speed: 3.0ms preprocess, 72.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 chair, 1 book, 83.4ms\n",
      "Speed: 3.1ms preprocess, 83.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 73.0ms\n",
      "Speed: 3.6ms preprocess, 73.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 74.9ms\n",
      "Speed: 3.6ms preprocess, 74.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 70.8ms\n",
      "Speed: 3.3ms preprocess, 70.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 88.0ms\n",
      "Speed: 3.9ms preprocess, 88.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 76.8ms\n",
      "Speed: 2.9ms preprocess, 76.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 80.8ms\n",
      "Speed: 2.8ms preprocess, 80.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 69.8ms\n",
      "Speed: 2.7ms preprocess, 69.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 68.3ms\n",
      "Speed: 2.8ms preprocess, 68.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 80.7ms\n",
      "Speed: 4.2ms preprocess, 80.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 1 book, 85.7ms\n",
      "Speed: 3.0ms preprocess, 85.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 71.2ms\n",
      "Speed: 3.0ms preprocess, 71.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 69.9ms\n",
      "Speed: 4.0ms preprocess, 69.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 69.4ms\n",
      "Speed: 4.2ms preprocess, 69.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 89.3ms\n",
      "Speed: 3.9ms preprocess, 89.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 81.4ms\n",
      "Speed: 7.9ms preprocess, 81.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 81.2ms\n",
      "Speed: 3.8ms preprocess, 81.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 69.9ms\n",
      "Speed: 3.7ms preprocess, 69.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 85.1ms\n",
      "Speed: 3.5ms preprocess, 85.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 81.1ms\n",
      "Speed: 2.9ms preprocess, 81.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 75.2ms\n",
      "Speed: 2.7ms preprocess, 75.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 book, 95.4ms\n",
      "Speed: 4.0ms preprocess, 95.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 76.8ms\n",
      "Speed: 2.9ms preprocess, 76.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 chair, 70.4ms\n",
      "Speed: 2.9ms preprocess, 70.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 chair, 83.1ms\n",
      "Speed: 2.9ms preprocess, 83.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 67.0ms\n",
      "Speed: 3.4ms preprocess, 67.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 spoon, 1 oven, 77.1ms\n",
      "Speed: 3.2ms preprocess, 77.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 82.1ms\n",
      "Speed: 3.0ms preprocess, 82.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 65.7ms\n",
      "Speed: 2.8ms preprocess, 65.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 78.7ms\n",
      "Speed: 3.2ms preprocess, 78.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 71.8ms\n",
      "Speed: 2.9ms preprocess, 71.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 refrigerator, 76.5ms\n",
      "Speed: 4.4ms preprocess, 76.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 bottles, 82.6ms\n",
      "Speed: 2.9ms preprocess, 82.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bottle, 1 cup, 1 bowl, 1 dining table, 86.4ms\n",
      "Speed: 2.8ms preprocess, 86.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 bottle, 1 cup, 1 bowl, 1 potted plant, 1 dining table, 74.8ms\n",
      "Speed: 2.9ms preprocess, 74.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 chair, 1 dining table, 76.2ms\n",
      "Speed: 3.3ms preprocess, 76.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 car, 1 chair, 1 dining table, 118.0ms\n",
      "Speed: 3.4ms preprocess, 118.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 1 bowl, 1 chair, 2 dining tables, 79.2ms\n",
      "Speed: 3.2ms preprocess, 79.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 car, 1 cup, 1 dining table, 71.3ms\n",
      "Speed: 2.9ms preprocess, 71.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 2 bowls, 2 dining tables, 75.2ms\n",
      "Speed: 2.9ms preprocess, 75.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 car, 1 cup, 1 dining table, 65.8ms\n",
      "Speed: 2.7ms preprocess, 65.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 bowl, 1 dining table, 75.5ms\n",
      "Speed: 3.7ms preprocess, 75.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bowl, 70.6ms\n",
      "Speed: 2.9ms preprocess, 70.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 car, 1 cup, 2 bowls, 78.1ms\n",
      "Speed: 3.1ms preprocess, 78.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 1 dining table, 73.3ms\n",
      "Speed: 3.8ms preprocess, 73.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 car, 1 cup, 1 bowl, 1 dining table, 74.4ms\n",
      "Speed: 2.8ms preprocess, 74.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 1 bowl, 1 dining table, 71.9ms\n",
      "Speed: 4.0ms preprocess, 71.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 2 cars, 1 cup, 1 bowl, 1 dining table, 76.9ms\n",
      "Speed: 3.0ms preprocess, 76.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 cars, 1 cup, 1 bowl, 1 dining table, 93.9ms\n",
      "Speed: 4.1ms preprocess, 93.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3 cars, 1 cup, 1 dining table, 80.0ms\n",
      "Speed: 3.1ms preprocess, 80.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 cup, 1 dining table, 91.8ms\n",
      "Speed: 4.1ms preprocess, 91.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 car, 1 cup, 1 bowl, 1 dining table, 97.9ms\n",
      "Speed: 2.9ms preprocess, 97.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 car, 1 boat, 1 cup, 2 bowls, 1 dining table, 83.0ms\n",
      "Speed: 3.0ms preprocess, 83.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 cup, 1 bowl, 1 chair, 1 dining table, 82.8ms\n",
      "Speed: 2.9ms preprocess, 82.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 boat, 1 cup, 1 bowl, 1 chair, 1 dining table, 77.3ms\n",
      "Speed: 3.8ms preprocess, 77.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 handbag, 1 cup, 1 dining table, 78.6ms\n",
      "Speed: 3.7ms preprocess, 78.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 cup, 1 dining table, 69.4ms\n",
      "Speed: 3.2ms preprocess, 69.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 cup, 1 bowl, 77.0ms\n",
      "Speed: 3.8ms preprocess, 77.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 cup, 1 dining table, 68.9ms\n",
      "Speed: 2.8ms preprocess, 68.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 cup, 1 dining table, 78.7ms\n",
      "Speed: 2.9ms preprocess, 78.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 cup, 2 bowls, 1 dining table, 106.2ms\n",
      "Speed: 25.6ms preprocess, 106.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 cup, 83.8ms\n",
      "Speed: 2.9ms preprocess, 83.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 1 bowl, 91.0ms\n",
      "Speed: 3.1ms preprocess, 91.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 cup, 1 bowl, 86.9ms\n",
      "Speed: 3.6ms preprocess, 86.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 cup, 1 dining table, 75.4ms\n",
      "Speed: 3.4ms preprocess, 75.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 cup, 1 dining table, 68.6ms\n",
      "Speed: 4.2ms preprocess, 68.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 bowl, 1 dining table, 69.4ms\n",
      "Speed: 4.2ms preprocess, 69.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 2 bowls, 1 dining table, 92.2ms\n",
      "Speed: 3.0ms preprocess, 92.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 cups, 1 dining table, 76.3ms\n",
      "Speed: 3.7ms preprocess, 76.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 68.8ms\n",
      "Speed: 3.1ms preprocess, 68.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 microwave, 71.8ms\n",
      "Speed: 3.3ms preprocess, 71.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bowl, 71.1ms\n",
      "Speed: 3.5ms preprocess, 71.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 laptops, 1 microwave, 69.8ms\n",
      "Speed: 2.8ms preprocess, 69.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 laptops, 70.5ms\n",
      "Speed: 3.1ms preprocess, 70.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 tvs, 1 laptop, 87.2ms\n",
      "Speed: 4.0ms preprocess, 87.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 tvs, 1 laptop, 82.8ms\n",
      "Speed: 3.8ms preprocess, 82.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bed, 1 tv, 92.0ms\n",
      "Speed: 3.0ms preprocess, 92.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 tvs, 1 laptop, 83.8ms\n",
      "Speed: 3.2ms preprocess, 83.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 tvs, 79.2ms\n",
      "Speed: 3.0ms preprocess, 79.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bed, 1 tv, 1 laptop, 73.9ms\n",
      "Speed: 3.2ms preprocess, 73.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 tvs, 76.0ms\n",
      "Speed: 3.1ms preprocess, 76.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 tvs, 93.9ms\n",
      "Speed: 3.6ms preprocess, 93.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 tv, 91.4ms\n",
      "Speed: 3.9ms preprocess, 91.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 laptop, 86.0ms\n",
      "Speed: 2.9ms preprocess, 86.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 73.4ms\n",
      "Speed: 4.3ms preprocess, 73.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 tv, 92.5ms\n",
      "Speed: 3.8ms preprocess, 92.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 suitcase, 1 laptop, 79.6ms\n",
      "Speed: 3.6ms preprocess, 79.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 laptop, 74.6ms\n",
      "Speed: 3.9ms preprocess, 74.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 tv, 1 laptop, 91.2ms\n",
      "Speed: 3.1ms preprocess, 91.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 laptop, 88.0ms\n",
      "Speed: 3.1ms preprocess, 88.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 suitcase, 1 tv, 83.7ms\n",
      "Speed: 2.9ms preprocess, 83.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 traffic light, 68.0ms\n",
      "Speed: 3.0ms preprocess, 68.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 traffic light, 67.3ms\n",
      "Speed: 3.2ms preprocess, 67.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 traffic light, 2 suitcases, 87.0ms\n",
      "Speed: 2.9ms preprocess, 87.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 traffic light, 1 suitcase, 75.4ms\n",
      "Speed: 3.0ms preprocess, 75.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 traffic light, 3 suitcases, 76.7ms\n",
      "Speed: 3.5ms preprocess, 76.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 traffic light, 1 tie, 1 suitcase, 1 cup, 70.5ms\n",
      "Speed: 2.8ms preprocess, 70.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 suitcase, 80.0ms\n",
      "Speed: 4.0ms preprocess, 80.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 2 suitcases, 69.9ms\n",
      "Speed: 2.9ms preprocess, 69.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 suitcases, 1 cup, 69.0ms\n",
      "Speed: 3.9ms preprocess, 69.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 suitcases, 83.4ms\n",
      "Speed: 2.9ms preprocess, 83.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 76.3ms\n",
      "Speed: 3.4ms preprocess, 76.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 71.7ms\n",
      "Speed: 3.1ms preprocess, 71.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 refrigerator, 90.8ms\n",
      "Speed: 3.9ms preprocess, 90.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 74.1ms\n",
      "Speed: 3.0ms preprocess, 74.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 66.2ms\n",
      "Speed: 3.6ms preprocess, 66.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 85.0ms\n",
      "Speed: 4.6ms preprocess, 85.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 suitcase, 69.9ms\n",
      "Speed: 3.2ms preprocess, 69.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 81.7ms\n",
      "Speed: 3.2ms preprocess, 81.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.1ms\n",
      "Speed: 3.6ms preprocess, 74.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 tv, 1 laptop, 70.7ms\n",
      "Speed: 3.7ms preprocess, 70.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 laptop, 69.7ms\n",
      "Speed: 3.4ms preprocess, 69.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 93.5ms\n",
      "Speed: 3.1ms preprocess, 93.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 82.2ms\n",
      "Speed: 3.8ms preprocess, 82.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 spoon, 1 dining table, 72.0ms\n",
      "Speed: 2.7ms preprocess, 72.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 dining table, 73.8ms\n",
      "Speed: 3.1ms preprocess, 73.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 bowl, 98.4ms\n",
      "Speed: 2.8ms preprocess, 98.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 spoon, 1 bowl, 1 dining table, 81.8ms\n",
      "Speed: 3.9ms preprocess, 81.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bowl, 1 dining table, 83.5ms\n",
      "Speed: 3.0ms preprocess, 83.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 bowl, 73.6ms\n",
      "Speed: 3.0ms preprocess, 73.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cups, 1 bowl, 73.6ms\n",
      "Speed: 2.9ms preprocess, 73.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bowl, 86.4ms\n",
      "Speed: 3.0ms preprocess, 86.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bowl, 77.6ms\n",
      "Speed: 2.9ms preprocess, 77.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cups, 1 bowl, 62.8ms\n",
      "Speed: 4.3ms preprocess, 62.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bowl, 83.1ms\n",
      "Speed: 4.0ms preprocess, 83.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bowl, 76.7ms\n",
      "Speed: 3.1ms preprocess, 76.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 spoon, 1 bowl, 73.5ms\n",
      "Speed: 3.0ms preprocess, 73.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bowl, 67.0ms\n",
      "Speed: 2.9ms preprocess, 67.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bowl, 84.8ms\n",
      "Speed: 3.0ms preprocess, 84.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 bowl, 1 dining table, 78.4ms\n",
      "Speed: 3.6ms preprocess, 78.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bowl, 78.8ms\n",
      "Speed: 3.2ms preprocess, 78.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 bowl, 91.6ms\n",
      "Speed: 3.8ms preprocess, 91.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 bowl, 1 tv, 76.5ms\n",
      "Speed: 3.0ms preprocess, 76.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 76.6ms\n",
      "Speed: 3.1ms preprocess, 76.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 87.8ms\n",
      "Speed: 3.4ms preprocess, 87.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 83.6ms\n",
      "Speed: 3.1ms preprocess, 83.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 80.7ms\n",
      "Speed: 2.8ms preprocess, 80.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 77.6ms\n",
      "Speed: 3.7ms preprocess, 77.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 84.2ms\n",
      "Speed: 3.8ms preprocess, 84.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 70.3ms\n",
      "Speed: 2.7ms preprocess, 70.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 78.0ms\n",
      "Speed: 4.2ms preprocess, 78.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 76.7ms\n",
      "Speed: 2.8ms preprocess, 76.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 bus, 1 train, 88.8ms\n",
      "Speed: 3.0ms preprocess, 88.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 82.4ms\n",
      "Speed: 2.9ms preprocess, 82.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 83.9ms\n",
      "Speed: 3.0ms preprocess, 83.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 bottles, 1 chair, 75.6ms\n",
      "Speed: 3.8ms preprocess, 75.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 bottle, 1 cup, 1 chair, 1 cell phone, 73.3ms\n",
      "Speed: 3.6ms preprocess, 73.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 chair, 74.1ms\n",
      "Speed: 3.3ms preprocess, 74.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 1 cell phone, 86.9ms\n",
      "Speed: 4.0ms preprocess, 86.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 chair, 1 cell phone, 75.4ms\n",
      "Speed: 3.8ms preprocess, 75.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 1 chair, 1 cell phone, 84.9ms\n",
      "Speed: 3.3ms preprocess, 84.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 chair, 1 cell phone, 94.8ms\n",
      "Speed: 2.9ms preprocess, 94.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 bottle, 1 chair, 1 laptop, 77.3ms\n",
      "Speed: 3.1ms preprocess, 77.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 chair, 90.1ms\n",
      "Speed: 3.3ms preprocess, 90.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 70.3ms\n",
      "Speed: 3.1ms preprocess, 70.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 chair, 84.8ms\n",
      "Speed: 3.0ms preprocess, 84.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 1 chair, 85.5ms\n",
      "Speed: 3.1ms preprocess, 85.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 1 chair, 81.1ms\n",
      "Speed: 2.9ms preprocess, 81.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 1 cell phone, 70.4ms\n",
      "Speed: 2.8ms preprocess, 70.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 chair, 68.3ms\n",
      "Speed: 4.0ms preprocess, 68.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 1 chair, 75.4ms\n",
      "Speed: 3.0ms preprocess, 75.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 1 knife, 1 chair, 1 cell phone, 93.1ms\n",
      "Speed: 3.8ms preprocess, 93.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 backpack, 1 bottle, 1 knife, 1 chair, 83.3ms\n",
      "Speed: 2.9ms preprocess, 83.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 1 chair, 70.4ms\n",
      "Speed: 3.0ms preprocess, 70.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 2 chairs, 96.3ms\n",
      "Speed: 3.0ms preprocess, 96.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 knife, 1 chair, 86.5ms\n",
      "Speed: 2.8ms preprocess, 86.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 backpack, 1 bottle, 1 chair, 84.8ms\n",
      "Speed: 3.7ms preprocess, 84.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 chair, 76.9ms\n",
      "Speed: 3.2ms preprocess, 76.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 chair, 124.9ms\n",
      "Speed: 3.1ms preprocess, 124.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 chair, 83.8ms\n",
      "Speed: 3.0ms preprocess, 83.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 cup, 1 chair, 74.2ms\n",
      "Speed: 3.8ms preprocess, 74.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 bottle, 1 cup, 1 tv, 79.1ms\n",
      "Speed: 3.5ms preprocess, 79.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 dining table, 85.0ms\n",
      "Speed: 3.0ms preprocess, 85.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cell phone, 75.0ms\n",
      "Speed: 2.6ms preprocess, 75.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 remote, 69.9ms\n",
      "Speed: 3.3ms preprocess, 69.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bottle, 1 cup, 1 cell phone, 92.0ms\n",
      "Speed: 3.1ms preprocess, 92.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 93.3ms\n",
      "Speed: 3.5ms preprocess, 93.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 79.4ms\n",
      "Speed: 3.8ms preprocess, 79.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 84.9ms\n",
      "Speed: 3.7ms preprocess, 84.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 90.0ms\n",
      "Speed: 2.8ms preprocess, 90.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 67.5ms\n",
      "Speed: 3.8ms preprocess, 67.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 92.9ms\n",
      "Speed: 4.0ms preprocess, 92.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 80.6ms\n",
      "Speed: 3.0ms preprocess, 80.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 74.5ms\n",
      "Speed: 2.7ms preprocess, 74.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 68.2ms\n",
      "Speed: 3.9ms preprocess, 68.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cups, 95.1ms\n",
      "Speed: 3.8ms preprocess, 95.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 89.9ms\n",
      "Speed: 2.8ms preprocess, 89.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 81.4ms\n",
      "Speed: 3.6ms preprocess, 81.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 80.1ms\n",
      "Speed: 3.7ms preprocess, 80.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 75.3ms\n",
      "Speed: 3.3ms preprocess, 75.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 89.0ms\n",
      "Speed: 3.0ms preprocess, 89.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 78.5ms\n",
      "Speed: 2.9ms preprocess, 78.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 2 chairs, 1 dining table, 73.6ms\n",
      "Speed: 3.3ms preprocess, 73.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 chair, 70.0ms\n",
      "Speed: 4.5ms preprocess, 70.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bottle, 1 cup, 2 chairs, 2 laptops, 1 cell phone, 84.3ms\n",
      "Speed: 2.8ms preprocess, 84.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 1 chair, 1 vase, 81.9ms\n",
      "Speed: 3.0ms preprocess, 81.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 1 chair, 69.2ms\n",
      "Speed: 3.1ms preprocess, 69.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 72.5ms\n",
      "Speed: 3.1ms preprocess, 72.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 95.5ms\n",
      "Speed: 4.0ms preprocess, 95.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 1 chair, 82.4ms\n",
      "Speed: 3.0ms preprocess, 82.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 1 chair, 74.8ms\n",
      "Speed: 2.7ms preprocess, 74.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 cup, 1 laptop, 81.2ms\n",
      "Speed: 3.3ms preprocess, 81.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cell phone, 92.3ms\n",
      "Speed: 2.9ms preprocess, 92.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 76.7ms\n",
      "Speed: 2.8ms preprocess, 76.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 1 chair, 1 tv, 85.9ms\n",
      "Speed: 3.5ms preprocess, 85.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 1 chair, 66.2ms\n",
      "Speed: 3.9ms preprocess, 66.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 1 chair, 79.3ms\n",
      "Speed: 3.2ms preprocess, 79.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 132.4ms\n",
      "Speed: 3.3ms preprocess, 132.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 2 cups, 90.7ms\n",
      "Speed: 3.1ms preprocess, 90.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 cup, 86.2ms\n",
      "Speed: 3.3ms preprocess, 86.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 74.4ms\n",
      "Speed: 3.8ms preprocess, 74.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 cup, 74.2ms\n",
      "Speed: 3.0ms preprocess, 74.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 cup, 93.0ms\n",
      "Speed: 3.0ms preprocess, 93.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 2 cups, 85.1ms\n",
      "Speed: 2.9ms preprocess, 85.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 cup, 79.1ms\n",
      "Speed: 3.0ms preprocess, 79.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 cup, 75.2ms\n",
      "Speed: 3.0ms preprocess, 75.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 cup, 68.9ms\n",
      "Speed: 3.5ms preprocess, 68.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 cup, 75.4ms\n",
      "Speed: 3.6ms preprocess, 75.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 tie, 1 cup, 68.2ms\n",
      "Speed: 3.0ms preprocess, 68.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 cup, 67.5ms\n",
      "Speed: 2.8ms preprocess, 67.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 1 cup, 86.0ms\n",
      "Speed: 3.2ms preprocess, 86.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 cup, 71.5ms\n",
      "Speed: 3.2ms preprocess, 71.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 cup, 83.4ms\n",
      "Speed: 2.9ms preprocess, 83.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1 cup, 79.3ms\n",
      "Speed: 3.7ms preprocess, 79.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 cup, 77.8ms\n",
      "Speed: 2.9ms preprocess, 77.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 tie, 1 cup, 65.6ms\n",
      "Speed: 2.9ms preprocess, 65.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 129.7ms\n",
      "Speed: 4.2ms preprocess, 129.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 87.3ms\n",
      "Speed: 3.0ms preprocess, 87.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 92.4ms\n",
      "Speed: 2.9ms preprocess, 92.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 cup, 1 tv, 82.1ms\n",
      "Speed: 4.0ms preprocess, 82.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 cup, 1 tv, 84.5ms\n",
      "Speed: 2.9ms preprocess, 84.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 81.1ms\n",
      "Speed: 2.9ms preprocess, 81.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 76.6ms\n",
      "Speed: 3.7ms preprocess, 76.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 tv, 1 oven, 67.7ms\n",
      "Speed: 3.8ms preprocess, 67.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 73.0ms\n",
      "Speed: 4.0ms preprocess, 73.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Video telah disimpan sebagai detected_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "model = YOLO('yolov8n.pt')  \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "output_filename = 'detected_video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') \n",
    "out = cv2.VideoWriter(output_filename, fourcc, 20.0, (frame_width, frame_height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    results = model(frame)\n",
    "\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    cv2.imshow(\"Webcam\", annotated_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Video telah disimpan sebagai\", output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 01:05:35.007 python[75701:1450240] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video telah disimpan sebagai video.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8.pt')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "output_filename = 'video.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') \n",
    "out = cv2.VideoWriter(output_filename, fourcc, 20.0, (frame_width, frame_height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    results = model(frame)\n",
    "\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "    cv2.imshow(\"Webcam\", annotated_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Video telah disimpan sebagai\", output_filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
